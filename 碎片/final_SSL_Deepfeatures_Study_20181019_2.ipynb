{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import scipy.io as scio\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = False\n",
    "device = torch.device('cuda' if use_gpu else 'cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = torchvision.models.resnet50(pretrained=True)\n",
    "model_resnet = model_resnet.to(device)\n",
    "# model_resnetCifar10 = \n",
    "model_resFeats = nn.Sequential(*list(model_resnet.children()))[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:/Users/admin/Desktop/data/cifar10.mat'\n",
    "batch_size = 8\n",
    "numLabels = 800\n",
    "num_iteration = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samplesForOneExperiment(data_path,numLabels):\n",
    "    dataCifar10 = scio.loadmat(data_path)\n",
    "    xTr = dataCifar10['xTr'].astype('float')\n",
    "    yTr = dataCifar10['yTr'].astype('float')\n",
    "    xTe = dataCifar10['xTe'].astype('float')\n",
    "    yTe = dataCifar10['yTe'].astype('float')\n",
    "    xTmp1, xTmp2, yTmp1, yTmp2 = train_test_split(xTr, yTr, test_size=35000, random_state=42, shuffle=True)\n",
    "    xTr1, xUn1, yTr1, yUn1 = train_test_split(xTmp1, yTmp1, test_size=len(yTmp1)-numLabels, random_state=500, shuffle=True)\n",
    "    return xTr1,yTr1, xUn1, yUn1, xTe, yTe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_feature_extractor(model,dataX):\n",
    "    feats = []\n",
    "    model.eval()\n",
    "    x_images = dataX.reshape((-1,3,32,32))\n",
    "    x_images = x_images.transpose((0,2,3,1))\n",
    "    num_image = np.shape(x_images)[0]\n",
    "    for i in range(num_image):\n",
    "        x_images_1 = Image.fromarray(np.uint8(x_images[i,:,:,:]))\n",
    "        x_images_1 = data_transforms(x_images_1)\n",
    "        feat_tmp = model(torch.unsqueeze(x_images_1,0))\n",
    "        feat_tmp = feat_tmp.detach().numpy()\n",
    "        feat_tmp = np.squeeze(feat_tmp)\n",
    "        if i==0:\n",
    "            feats = feat_tmp\n",
    "        else:\n",
    "            feats = np.vstack((feats,feat_tmp))\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(xTr,yTr,model,criterion, optimizer, scheduler, num_epochs=25):\n",
    "    model.train()\n",
    "    since = time.time()\n",
    "    dataset_sizes = len(yTr)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 30)\n",
    "\n",
    "        scheduler.step()\n",
    "        model.train(True)  # 设置 model 为训练 (training) 模式\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "            # 遍历数据\n",
    "        mini_batches = random_mini_batches(xTr,yTr,mini_batch_size=batch_size,seed=0)\n",
    "        for batch_idx, (inputs, labels) in enumerate(mini_batches):\n",
    "            x_images = inputs.reshape((-1,3,32,32))\n",
    "            x_images = x_images.transpose((0,2,3,1))\n",
    "            num_image = np.shape(x_images)[0]\n",
    "            inputs1 = torch.zeros(num_image,3,224,224,dtype = torch.float)\n",
    "            for i in range(num_image):\n",
    "                x_images_1 = Image.fromarray(np.uint8(x_images[i,:,:,:]))\n",
    "                x_images_1 = data_transforms(x_images_1)\n",
    "                inputs1[i,:,:,:] = x_images_1\n",
    "            \n",
    "            # 用 Variable 包装输入数据\n",
    "            if use_gpu:\n",
    "                inputs = Variable(inputs1.cuda().cuda())\n",
    "                labels = Variable(torch.from_numpy(labels).cuda())\n",
    "            else:\n",
    "                inputs, labels = Variable(inputs1), Variable(torch.from_numpy(labels))\n",
    "            \n",
    "            # 设置梯度参数为 0\n",
    "            optimizer.zero_grad()\n",
    "            # 正向传递\n",
    "            #print( inputs.size())\n",
    "            outputs = model(inputs)\n",
    "            #print( outputs.size())\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            #print(preds.size())\n",
    "            #print(labels.size())\n",
    "            #print(outputs)\n",
    "            labels = torch.Tensor.long(labels)\n",
    "            #print(labels.dtype)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # 如果是训练阶段, 向后传递和优化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # 统计\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        running_corrects = running_corrects.float()\n",
    "        epoch_loss = running_loss / dataset_sizes\n",
    "        epoch_acc = running_corrects / dataset_sizes\n",
    "\n",
    "        print('Loss: {:.4f} Acc: {:.4f}'.format( epoch_loss, epoch_acc))\n",
    "\n",
    "            # 深拷贝 model\n",
    "        if  epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    # 加载最佳模型的权重\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X,Y,mini_batch_size= 16, seed=0):\n",
    "\n",
    "    np.random.seed(seed) #指定随机种子\n",
    "    m = X.shape[0]\n",
    "    mini_batches = []\n",
    "\n",
    "    #第一步：打乱顺序\n",
    "    permutation = list(np.random.permutation(m)) #它会返回一个长度为m的随机数组，且里面的数是0到m-1\n",
    "    shuffled_X = X[permutation,:]   #将每一列的数据按permutation的顺序来重新排列。\n",
    "    shuffled_Y = np.squeeze(Y[permutation])\n",
    "\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) #把你的训练集分割成多少份,请注意，如果值是99.99，那么返回值是99，剩下的0.99会被舍弃\n",
    "#    print(m)\n",
    "#    print(num_complete_minibatches)\n",
    "#    print(np.shape(shuffled_Y))\n",
    "    for k in range(0,num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size:(k+1)*mini_batch_size,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch = (mini_batch_X,mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    #如果训练集的大小刚好是mini_batch_size的整数倍，那么这里已经处理完了\n",
    "    #如果训练集的大小不是mini_batch_size的整数倍，那么最后肯定会剩下一些，我们要把它处理了\n",
    "    if m % mini_batch_size != 0:\n",
    "        #获取最后剩余的部分\n",
    "        mini_batch_X = shuffled_X[mini_batch_size * num_complete_minibatches:,:]\n",
    "        mini_batch_Y = shuffled_Y[mini_batch_size * num_complete_minibatches:]\n",
    "\n",
    "        mini_batch = (mini_batch_X,mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inFeats = model_resnet.fc.in_features\n",
    "model_resnet.fc = nn.Linear(num_inFeats, 10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_resnet.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.876901\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.9745 Acc: 0.7212\n",
      "\n",
      "Training complete in 5m 44s\n",
      "Best val Acc: 0.721250\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.890352\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.2550 Acc: 0.9362\n",
      "\n",
      "Training complete in 5m 58s\n",
      "Best val Acc: 0.936250\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.900282\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.0713 Acc: 0.9912\n",
      "\n",
      "Training complete in 5m 58s\n",
      "Best val Acc: 0.991250\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.899789\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.0307 Acc: 0.9987\n",
      "\n",
      "Training complete in 5m 59s\n",
      "Best val Acc: 0.998750\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.899930\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.0152 Acc: 1.0000\n",
      "\n",
      "Training complete in 5m 58s\n",
      "Best val Acc: 1.000000\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.900352\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.0102 Acc: 1.0000\n",
      "\n",
      "Training complete in 5m 59s\n",
      "Best val Acc: 1.000000\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.900282\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.0081 Acc: 1.0000\n",
      "\n",
      "Training complete in 5m 55s\n",
      "Best val Acc: 1.000000\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.900070\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.0071 Acc: 1.0000\n",
      "\n",
      "Training complete in 5m 55s\n",
      "Best val Acc: 1.000000\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.900000\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.0070 Acc: 1.0000\n",
      "\n",
      "Training complete in 5m 55s\n",
      "Best val Acc: 1.000000\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.899930\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.0069 Acc: 1.0000\n",
      "\n",
      "Training complete in 5m 54s\n",
      "Best val Acc: 1.000000\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.900000\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.0068 Acc: 1.0000\n",
      "\n",
      "Training complete in 5m 56s\n",
      "Best val Acc: 1.000000\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.900070\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.0067 Acc: 1.0000\n",
      "\n",
      "Training complete in 5m 55s\n",
      "Best val Acc: 1.000000\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.900070\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.0066 Acc: 1.0000\n",
      "\n",
      "Training complete in 5m 54s\n",
      "Best val Acc: 1.000000\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.900141\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.0065 Acc: 1.0000\n",
      "\n",
      "Training complete in 5m 54s\n",
      "Best val Acc: 1.000000\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.900000\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.0064 Acc: 1.0000\n",
      "\n",
      "Training complete in 5m 56s\n",
      "Best val Acc: 1.000000\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.900070\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.0064 Acc: 1.0000\n",
      "\n",
      "Training complete in 5m 54s\n",
      "Best val Acc: 1.000000\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.900070\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.0064 Acc: 1.0000\n",
      "\n",
      "Training complete in 5m 55s\n",
      "Best val Acc: 1.000000\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.900070\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.0064 Acc: 1.0000\n",
      "\n",
      "Training complete in 5m 54s\n",
      "Best val Acc: 1.000000\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.900070\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.0064 Acc: 1.0000\n",
      "\n",
      "Training complete in 5m 55s\n",
      "Best val Acc: 1.000000\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised Learning Accuracy:0.900070\n",
      "Epoch 0/0\n",
      "------------------------------\n",
      "Loss: 0.0064 Acc: 1.0000\n",
      "\n",
      "Training complete in 5m 53s\n",
      "Best val Acc: 1.000000\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    }
   ],
   "source": [
    "xTr,yTr,xUn,yUn,xTe,yTe = samplesForOneExperiment(data_path,numLabels)\n",
    "\n",
    "for iter1 in range(0,num_iteration):\n",
    "    # Extract data features, which takes a lot of time\n",
    "    xTr_features = CNN_feature_extractor(model_resFeats,xTr)\n",
    "    xUn_features = CNN_feature_extractor(model_resFeats,xUn)\n",
    "    # SSL, obtain psudo-labels of unlabeled samples\n",
    "    dataX = np.vstack((xTr_features,xUn_features)).astype(np.float64)\n",
    "    dataY = np.vstack((yTr,yUn))\n",
    "    numSamples = len(dataY)\n",
    "    ind_unlabeled = np.arange(numLabels,numSamples)\n",
    "    dataY[ind_unlabeled] = -1\n",
    "    cls=LabelSpreading(max_iter=150,kernel='rbf', gamma = 0.003)\n",
    "    cls.fit(preprocessing.scale(dataX),dataY.ravel())\n",
    "    predicted_labels=cls.transduction_[ind_unlabeled]\n",
    "    print(\"Semi-Supervised Learning Accuracy:%f\"%metrics.accuracy_score(yUn,predicted_labels))\n",
    "    # Finetune the network using given labels and psudo-labels\n",
    "    model_resnet = train_model(xTr,yTr,model_resnet, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=1)\n",
    "    model_resnet = train_model(xUn,predicted_labels,model_resnet, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
