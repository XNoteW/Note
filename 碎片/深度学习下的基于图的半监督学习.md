[^1]: Wikipedia.One-hot.[DB/OL].https://en.wikipedia.org/wiki/One-hot.

[^2]: 朱文涛, 袁勇.Python计算机视觉编程[M].北京：人民邮电出版社，2014.7:152-154.

[^3]: Zhu X, Ghahramani Z.Learning from labeled and unlabeled data with label propagation.Pittsburgh: Carnegie Mellon University, 2002

[^4]: Chapelle O, Scholkopf B.Semi-Supervised Learning.MA: MIT Press, 2006

[^5]: Zhou D, Bousquet O, Lal T N, et al. Learning with Local and Global Consistency[C]. neural information processing systems, 2004: 321-328.

[^6]: 匡继昌.实分析与泛函分析[M].北京:高等教育出版社.2002.8.

# 基于 CNN 提取特征进行标签传播

摘要: a

关键词: Label Propagation; LDA; PCA; SVM; CNN

# 1 相关工作

## 1.1 研究的背景与意义

## 1.2 符号说明

下面的 $\hat{}$ 均表示预测值. 有数据集 $T = \{(\alpha^i,y^i)\}_{i=1}^l \cup \{\alpha^j\}_{j=l+1}^m$. 一般称 $\mathcal{X} = \text{span}(\alpha^1, \alpha^2, \cdots, \alpha^m)$ 为**数据空间**或**输入空间**; 称 $\mathcal{Y} = \{y^1,y^2,\cdots, y^l\}$ 为**标记空间**或**输出空间**.

其中, 对于 $1\leq i\leq l$, $\alpha^i$ 有标签 $y^i$, 对于分类任务, $y^i \in \{1, 2, \cdots, C\}$ 或者 $y^i \in {\{0,1\}}^C$ (当 $y^i$ 为类别 $k \in \{1, 2, \cdots, C\}$ 时, $y_i$ 的第 $k$ 个元素是 $1$, 其余元素均为 $0$, 即 One-hot[^1] 编码的形式), 其中 $C$ 为类别数目.

对于 $∀ 1 \leq i\leq m$, 人们通过一些手段可以获得数据 $\alpha^i$ 的数字特征 $x$. 以一张彩色图片 $\alpha$ 为例, 人们可以获悉其像素的三维数组形式, 然后, 人们通过分析该数组, 建立一些模型来分析原数据的信息.

本文仅仅考虑图像分类任务. 具体介绍如下:

对于 $∀ 1 \leq i \leq m,$ 有一个映射 $g$, 使得 $x^i = g(\alpha^i) \in \mathbb{R}^n$, 称 $\mathcal{F} = \text{span}(x^1,x^1,\cdots,x^m)$ 为**特征空间**.

下面考虑映射 $f: \mathcal{F} \rightarrow \mathcal{Y}$:

令 $f(x^i) = (x^i)^Tw + b$, 则记

$$
X_l = \begin{pmatrix}
(x^1)^T\\(x^2)^T\\ \vdots \\(x^l)^T
\end{pmatrix};
X_u = \begin{pmatrix}
(x^{l+1})^T\\(x^{l+2})^T\\ \vdots \\(x^m)^T
\end{pmatrix};
Y_l = \begin{pmatrix}
(y^1)^T\\(y^2)^T\\ \vdots \\(y^l)^T
\end{pmatrix}
$$

则有 $\hat{Y}_l = X_lw + b$ 成立 (一般被称为**线性模型**).

为了求解的便利, 记
$$
\Phi_l = (X_l,1); \theta = \begin{pmatrix}
w\\b
\end{pmatrix}
$$
则 $\hat{Y}_l = \Phi_l \theta$.

考虑 $\{x^j\}_{j=l+1}^m$, 则记

$$
Y_u = \begin{pmatrix}
(y^{l+1})^T\\(y^{l+2})^T\\ \vdots \\(y^m)^T
\end{pmatrix};
X = \begin{pmatrix}
X_l\\X_u
\end{pmatrix};
Y = \begin{pmatrix}
Y_l\\Y_u
\end{pmatrix};
\hat{Y} = \begin{pmatrix}
\hat{Y_l} \\\hat{Y_u} 
\end{pmatrix}
$$

其中 $Y_u$ 为 $X_u$ 对应的虚拟标签 (实际上是未知的, 仅仅作为一个未知变量来方便问题的陈述).

### 1.2.1 线性回归

可以说**线性回归**是大多数机器学习模型的基石. 模型的求解过程便是最小化损失函数 $J_{\theta} = \frac{1}{2}||\hat{Y}_l - Y_l||^2$ 的过程. 该损失函数可以使用**最小二乘法**求得其最小二乘解为 $\theta^* = (\Phi_l^T\Phi_l)^{\dagger}\Phi_l^TY_l$, 但若 $X_l$ 的维度很高, 计算机内存开销会很大, 此时可以选择使用**随机梯度下降法**来求解.

### 1.2.2 分类任务

分类任务可以看作是广义线性模型与分类函数的复合, 即 $h = g(f(x))$, 这里 $f$ 是广义线性模型; $g$ 是分类函数, 当 $y^i \in \{1, 2, \cdots, C\}$ 时, 一般采用一对其他 ($OvR$) 或 一对一($OvO$) 的分类策略, 获得多个二分类器 $g$, 测试时选择表现比较好的分类器即可.

当然, 还可以采用基于概率的方法进行分类, 此时考虑线性映射

$$
\begin{aligned}
f: \;&\mathcal{F} \rightarrow {\{y: y\in [0,1]^C\}}\\
&x \mapsto y
\end{aligned}
$$

这里 $f(x)$ 可以理解为数据对于各个类别的置信程度. 最终的预测结果是预测概率最大的分量对应的类别, 即 $\hat{y}^i = \arg\max f(x)$

## 1.3 CNN

前面提到的线性模型 $f(\alpha) = x^Tw + b$, 在神经网络模型中可以这样理解:

- 将 $f_1, f_2, \cdots, f_n$ 看作神经网络的输出层的 $n$ 个神经元, 便可生成 $x = (f_1(\alpha), f_2(\alpha), \cdots, f_n(\alpha))^T = (x_1,x_2,\cdots,x_n)^T$

这样, $\hat{F}(\alpha)$ 可以是 CNN 等网络结构.

## 1.4 标签传播

下面构建一个图 ${\mathcal{G}} = (T, E)$, 其中, 边集为 $E = \{e_{ij}: e_{ij} = (x^i,x^j)\}$ ($ 1 \leq i,j \leq m$), 可表示为一个亲和矩阵（affinity matrix）(矩阵中每个元素表示两两之间的相似性分数[^3]) $W$，令 $d_j = \displaystyle\sum_{j=1}^m (W)_{ij}$, 而对角矩阵 $D = \operatorname{diag}(d_1, d_2, \cdots, d_m)$ 被称为**度矩阵**, 则由 $\Delta = D- W$ ($\Delta$ 被称为拉普拉斯矩阵), 则可以定义 $f$ 的能量函数为

$$
    E(f) = \frac{1}{2} {\displaystyle\sum_{i=1}^{m}\sum_{j=1}^m (W)_{ij} (f(x^i) - f(x^j))^2}
= \langle \hat{Y}, \Delta \hat{Y} \rangle
$$

将矩阵分块, 得到

$$
D = \begin{bmatrix}
    D_{ll} & D_{lu}\\
    D_{ul} & D_{uu}
\end{bmatrix};
W = \begin{bmatrix}
    W_{ll} & W_{lu}\\
    W_{ul} & W_{uu}
\end{bmatrix};\Delta = \begin{bmatrix}
    \Delta_{ll} & \Delta_{lu}\\
    \Delta_{ul} & \Delta_{uu}
\end{bmatrix}
$$

故而有

$$
\begin{aligned}
\langle \hat{Y}, \Delta \hat{Y} \rangle
& = \text{Tr}(\hat{Y}^T \Delta \hat{Y})\\
& = \langle \hat{Y_l}, \Delta_{ll} \hat{Y_l} \rangle + \langle \hat{Y_u}, \Delta_{uu} \hat{Y_u} \rangle + 2 \langle \hat{Y_u}, \Delta_{ul} \hat{Y_l} \rangle
\end{aligned}
$$

这里 $\hat{Y_l} = Y_l$, 故而原问题可以转换为:

$$
\underset{\hat{Y_u}}{\arg\min}\;E(f) = \underset{\hat{Y_u}}{\arg\min}\;\langle \hat{Y_u}, \Delta_{uu} \hat{Y_u} \rangle + 2 \langle \hat{Y_u}, \Delta_{ul} Y_l \rangle
$$

因此，由 $\frac{\partial E(f)}{\partial \hat{Y_u}} = 0$, 可知 $\Delta_{uu}\hat{Y_u} = W_{ul} Y_l$.

令 $P = WD^{-1}$, 则 $P$ 被称为**转移概率矩阵**. 类比 $W$ 的分块方式, 对 $P$ 做一样的处理, 则有

 $\hat{Y_u} = P_{uu}\hat{Y_u} + P_{ul} Y_l$.

 若数据集的样本数 $m \leq 5000$, 可以直接通过解析式 $\hat{Y_u} = (I - P_{uu})^{-1} P_{ul} Y_l$ 直接对无标签数据进行打标; 若 $m > 5000$, 则考虑迭代公式 $(\hat{Y_u})_{k+1} = P_{uu}(\hat{Y_u})_{k} + P_{ul} Y_l$ (其中 $k=0,1,2,\cdots$), 来获得问题的解.

### 关于权重矩阵 $W$ 与 图的边

权重矩阵 $W$ 又被称为**相似度矩阵**, **邻接矩阵**或**亲和矩阵**, $(W)_{ij}$ 作为样本点相似度的计算方法, 可以是样本数据 $x_i$ 与 $x_j$ 之间的距离 (如欧式距离, 马氏距离, 夹角余弦), 也可以是带宽为 $\sigma$ 的高斯核: $(W)_{ij} = exp(- \frac{||x_i - x_j||^2}{2\sigma^2}) \in [0, 1]$. 根据图中边的连接情况, 图大致可以分为全连接图 (Fully connected graph, 任意两个节点之间都有边存在), $k$ 近邻图 ($kNN$, k nearest neighbours graph) 和 $\varepsilon NN$ 图.

$k$ 近邻图需要首先知晓任意节点的 $k$ 个近邻. 记 $x_i$ $(i=1, \cdots, n)$ 最近的 $k$ 个近邻为 $N(x_i)$, 注意 $x_i \in N(x_j)$, 不一定就有 $x_j \in N(x_i)$. 因此, 若 $x_i \in N(x_j)$ 或者 $x_j \in N(x_i)$ 当中任意一个成立, 那么 $x_i$ 与 $x_j$ 之间就有边连接. 若只是节点距离 $||x_i - x_j|| < \varepsilon$ 时, 才连接这两个点, 此时的图便是 $\varepsilon NN$ 图了.



令 $\mathcal{I}$ 表示输入数据 $x$ 的数据空间, 被称为**输入空间** (input space); $\mathcal{F} =\mathcal{I}^{**}$ 被称为**特征空间** (feature space).

若 $\mathcal{I}^*$ 是数域 $K$ (实数域 $\mathbb{R}$ 或复数域 $\mathbb{C}$) 上基底为 $\phi = (\phi_1, \phi_2, \cdots, \phi_n)$ 的线性空间, $\forall \alpha_i \in \mathcal{I}$, 其中 $1 \leq n \leq \infty$, 则存在唯一的坐标 $\theta = (\theta_1,\theta_2, \cdots,\theta_n)^T \in K^n$ 使得任意的 $f \in \mathcal{I}^*$ 有
$$
f = \displaystyle\sum_{i=1}^n \phi_i \theta_i = \phi \theta
$$


## 1.5 降维

降维是指提取高维的关键信息, 将其转换为易于计算的低维空间的问题进行求解的过程.

本文主要涉及两种降维方法, 无监督的 PCA 和有监督的 LDA.

### 1.4.1 PCA

PCA 中文叫做**主成分分析法**, 该方法将输入数据 $x$ 正交投影表示为 $z$, 目的是学习该正交变换, 使得降维后的数据尽可能的保留原数据空间的信息, 具体如下:

设编码函数为

$$
\begin{aligned}
f:\; & \mathbb{R}^n \rightarrow \mathbb{R}^p\\
&x \mapsto z
\end{aligned}
$$

解码函数为

$$
\begin{aligned}
g:\; & \mathbb{R}^p \rightarrow \mathbb{R}^n\\
&z \mapsto Dz
\end{aligned}
$$

其中 $p<n$, $D \in \mathbb{R}^{n \times p}, D^TD = I_p$, 通常 $D$ 被称为解码矩阵.

为了保留原始信息, 需要保证**最近重构性**, 即 $\displaystyle\min_D\sum_i ||x^i - Dz^i||$ 或者 **最大可分性**, 即 $\displaystyle\max_D ||XD||$.

求得问题的解为

$$
X^TXD = \lambda D
$$

这样, 将 $X$ 进行标准化处理, 那么 $X^TX$ 可以看作是样本的协方差矩阵. 对 $X^TX$ 进行特征值分解, 并最大的 $p$ 个特征值所对应的特征向量 $d_1,d_2,\cdots,d_p$, 则便可得到 $D^* = (d_1,d_2,\cdots,d_p)$. 因而, 对于任意的 $x^i$, 便可得到其低维表示 $z^i = D^Tx^i$.

### 1.4.2 LDA

LDA 中文叫做线性判别分析, LDA 的核心思想是设法将样本投影到一个超平面上, 且使得同类的样本投影后尽可能的接近而不同类的样本投影后尽可能的分开. LDA 与 PCA 十分相似, PCA 试图找到方差最大的主成分, 而 LDA 的目标是发现可以最优化分类的特征子空间. 与 PCA 类似, 需要将 $X$ 进行标准化处理, 然后将数据集按类别划分为 $\{C_1,C_2, \cdots, C_C\}$, 并计算各个子类所在集合的均值向量 $\mu_j, j= 1,2,\cdots, C$, 则不同类间尽可能分离等价于

$$
\displaystyle \max_W \sum_{i,j=1}^C ||W^T(\mu_i - \mu_j)||^2 =  \max_W W^T \sum_{i,j=1}^C (u_i-u_j)(u_i-u_j)^T W = \max_W W^T S_b W
$$

同类之间尽可能接近等价于

$$
\displaystyle \min_W \sum_{x \in C_i} ||W^T(x_i - \mu_i)||^2 = \min_W W^T \sum_{x \in C_i} (x-u_i)(x-u_i)^T W = \min_W W^T S_w W
$$

构造类间散度矩阵 $S_b$ 以及类内散度矩阵 $S_w$. 故而原问题便转换为以下优化问题:

$$
\displaystyle\max_W \frac{\text{Tr}(W^TS_bW)}{\text{Tr}(W^TS_wW)}
$$

其中 $W$ 相当于 PCA 中的 $D$, 被视为投影矩阵. 因而, 与 PCA 类似取 $S_w^{-1}S_b$ 前 $p$ 大的特征值所对应的特征向量组成 $W$, 最后使用 $W$ 将样本投影到新的特征子空间中.

# 实验与结果分析



# 总结与展望

# 参考文献
