[^1]: Wikipedia.One-hot.[DB/OL].https://en.wikipedia.org/wiki/One-hot.

[^2]: 朱文涛, 袁勇.Python计算机视觉编程[M].北京：人民邮电出版社，2014.7:152-154.

[^3]: Zhu X, Ghahramani Z.Learning from labeled and unlabeled data with label propagation.Pittsburgh: Carnegie Mellon University, 2002

[^4]: Chapelle O, Scholkopf B.Semi-Supervised Learning.MA: MIT Press, 2006

[^5]: Zhou D, Bousquet O, Lal T N, et al. Learning with Local and Global Consistency[C]. neural information processing systems, 2004: 321-328.

[^6]: 匡继昌.实分析与泛函分析[M].北京:高等教育出版社.2002.8.

[^7]: 概半图模型：原理与技术 / (美国) Kollcr D., (以) Friedman N. 著, 王飞跃, 韩素青译. 北京: 清华大学出版社, 2015.

# 基于 CNN 提取特征进行标签传播

摘要: a

关键词: Label Propagation; LDA; PCA; SVM; CNN

# 1 相关工作

## 1.1 研究的背景与意义

本文仅仅考虑图像分类任务. 所谓图像分类任务, 就是已有固定的分类标签集合, 对给定的图像预测它属于哪个分类标签 (或者给出属于一系列不同标签的可能性). 对于人类来说, 识别一张图片是否是一只 "狗" 是很简单的任务, 然而对于计算机来说就显得尤为困难. 比如一张彩色图片数据 $x$一般都是 $3$ 维数组, 数组元素的取值范围是 $0 \sim 255$ 的整数. 受拍摄视角, 图片尺寸, 光照条件等各种因素干扰对计算机来说 $x$ 的像素值会发生改变, 哪怕是一点轻微的色调扰动都会增加计算机对 $x$ 的识别准确度.

## 1.2 符号说明

下面的 $\hat{}$ 均表示预测值. 有数据集 $T = \{(\alpha^{(i)},y^{(i)})\}_{i=1}^l \cup \{\alpha^{(j)}\}_{j=l+1}^m$. 一般称 $\mathcal{X} = \text{span}(\alpha^{(1)}, \alpha^{(2)}, \cdots, \alpha^{(m)})$ 为**数据空间**或**输入空间**; 称 $\mathcal{Y} = \{y^{(1)},y^{(2)},\cdots, y^{(l)}\}$ 为**标记空间**或**输出空间**.

本文默认向量为列向量, 有 $y^{(i)} \in \{1, 2, \cdots, C\}$ 或者 $y^{(i)} \in {\{0,1\}}^C$ (当 $y^{(i)}$ 为类别 $k \in \{1, 2, \cdots, C\}$ 时, $y^{(i)}$ 的第 $k$ 个元素是 $1$, 其余元素均为 $0$, 即 One-hot[^1] 编码的形式), 其中 $C$ 为类别数目.
. 具体介绍如下:

对于 $∀ 1 \leq i \leq m,$ 有一个映射 $g$, 使得 $x^{(i)} = g(\alpha^{(i)}) \in \mathbb{R}^n$, 称 $\mathcal{F} = \text{span}(x^{(1)},x^{(2)},\cdots,x^{(m)})$ 为**特征空间**. 约定:

$$
\begin{cases}
X_l = (x^{(1)}, x^{(2)}, \cdots, x^{(l)})^T\\
X_u = (x^{(l+1)}, x^{(l+2)}, \cdots, x^{(m)})^T\\
Y_l = (y^{(1)}, y^{(2)}, \cdots, y^{(l)})^T
\end{cases}
$$

## 1.3 标签传播

下面构建一个图 ${\mathcal{G}} = (T, E)$, 其中, 边集为 $E = \{e_{ij}: e_{ij} = (x^{(i)},x^{(j)})\}$ ($ 1 \leq i,j \leq m$), 可表示为一个亲和矩阵（affinity matrix）(矩阵中每个元素表示两两之间的相似性分数[^3]) $W$，令 $d_j = \displaystyle\sum_{j=1}^m (W)_{ij}$, 而对角矩阵 $D = \operatorname{diag}(d_1, d_2, \cdots, d_m)$ 被称为**度矩阵**, 则由 $\Delta = D- W$ ($\Delta$ 被称为拉普拉斯矩阵), 则可以定义 $f$ 的能量函数为

$$
    E(f) = \frac{1}{2} {\displaystyle\sum_{i=1}^{m}\sum_{j=1}^m (W)_{ij} (f(x^i) - f(x^j))^2}
= \langle \hat{Y}, \Delta \hat{Y} \rangle
$$

将矩阵分块, 得到

$$
D = \begin{bmatrix}
    D_{ll} & D_{lu}\\
    D_{ul} & D_{uu}
\end{bmatrix};
W = \begin{bmatrix}
    W_{ll} & W_{lu}\\
    W_{ul} & W_{uu}
\end{bmatrix};\Delta = \begin{bmatrix}
    \Delta_{ll} & \Delta_{lu}\\
    \Delta_{ul} & \Delta_{uu}
\end{bmatrix}
$$

故而有

$$
\begin{aligned}
\langle \hat{Y}, \Delta \hat{Y} \rangle
& = \text{Tr}(\hat{Y}^T \Delta \hat{Y})\\
& = \langle \hat{Y_l}, \Delta_{ll} \hat{Y_l} \rangle + \langle \hat{Y_u}, \Delta_{uu} \hat{Y_u} \rangle + 2 \langle \hat{Y_u}, \Delta_{ul} \hat{Y_l} \rangle
\end{aligned}
$$

这里 $\hat{Y_l} = Y_l$, 故而原问题可以转换为:

$$
\underset{\hat{Y_u}}{\arg\min}\;E(f) = \underset{\hat{Y_u}}{\arg\min}\;\langle \hat{Y_u}, \Delta_{uu} \hat{Y_u} \rangle + 2 \langle \hat{Y_u}, \Delta_{ul} Y_l \rangle
$$

因此，由 $\frac{\partial E(f)}{\partial \hat{Y_u}} = 0$, 可知 $\Delta_{uu}\hat{Y_u} = W_{ul} Y_l$.

令 $P = WD^{-1}$, 则 $P$ 被称为**转移概率矩阵**. 类比 $W$ 的分块方式, 对 $P$ 做一样的处理, 则有

 $\hat{Y_u} = P_{uu}\hat{Y_u} + P_{ul} Y_l$.

 若数据集的样本数 $m \leq 5000$, 可以直接通过解析式 $\hat{Y_u} = (I - P_{uu})^{-1} P_{ul} Y_l$ 直接对无标签数据进行打标; 若 $m > 5000$, 则考虑迭代公式 $(\hat{Y_u})_{k+1} = P_{uu}(\hat{Y_u})_{k} + P_{ul} Y_l$ (其中 $k=0,1,2,\cdots$), 来获得问题的解.

其中, 权重矩阵 $W$ 又被称为**相似度矩阵**, **邻接矩阵**或**亲和矩阵**, $(W)_{ij}$ 作为样本点相似度的计算方法, 可以是样本数据 $x_i$ 与 $x_j$ 之间的距离 (如欧式距离, 马氏距离, 夹角余弦), 也可以是带宽为 $\sigma$ 的高斯核: $(W)_{ij} = exp(- \frac{||x_i - x_j||^2}{2\sigma^2}) \in [0, 1]$. 根据图中边的连接情况, 图大致可以分为全连接图 (Fully connected graph, 任意两个节点之间都有边存在), $k$ 近邻图 ($kNN$, k nearest neighbours graph) 和 $\varepsilon NN$ 图.

$k$ 近邻图需要首先知晓任意节点的 $k$ 个近邻. 记 $x_i$ $(i=1, \cdots, n)$ 最近的 $k$ 个近邻为 $N(x_i)$, 注意 $x_i \in N(x_j)$, 不一定就有 $x_j \in N(x_i)$. 因此, 若 $x_i \in N(x_j)$ 或者 $x_j \in N(x_i)$ 当中任意一个成立, 那么 $x_i$ 与 $x_j$ 之间就有边连接. 若只是节点距离 $||x_i - x_j|| < \varepsilon$ 时, 才连接这两个点, 此时的图便是 $\varepsilon NN$ 图了.

## 1.4 CNN

前面提到的线性模型 $f(\alpha) = x^Tw + b$, 在神经网络模型中可以这样理解:

- 将 $f_1, f_2, \cdots, f_n$ 看作神经网络的输出层的 $n$ 个神经元, 便可生成 $x = (f_1(\alpha), f_2(\alpha), \cdots, f_n(\alpha))^T = (x_1,x_2,\cdots,x_n)^T$

这样, $\hat{F}(\alpha)$ 可以是 CNN 等网络结构.

## 1.5 降维

降维是指提取高维的关键信息, 将其转换为易于计算的低维空间的问题进行求解的过程.

本文主要涉及两种降维方法, 无监督的 PCA 和有监督的 LDA.

### 1.5.1 PCA

PCA 中文叫做**主成分分析法**, 该方法将输入数据 $x$ 正交投影表示为 $z$, 目的是学习该正交变换, 使得降维后的数据尽可能的保留原数据空间的信息, 具体如下:

设编码函数为

$$
\begin{aligned}
f:\; & \mathbb{R}^n \rightarrow \mathbb{R}^p\\
&x \mapsto z
\end{aligned}
$$

解码函数为

$$
\begin{aligned}
g:\; & \mathbb{R}^p \rightarrow \mathbb{R}^n\\
&z \mapsto Bz
\end{aligned}
$$

其中 $p<n$, $B \in \mathbb{R}^{n \times p}, B^TB = I_p$, 通常 $B$ 被称为解码矩阵.

为了保留原始信息, 需要保证**最近重构性**, 即 $\displaystyle\min_B\sum_i ||x^i - Bz^i||$ 或者 **最大可分性**, 即 $\displaystyle\max_B ||XB||$.

求得问题的解为

$$
X^TXB = \lambda B
$$

这样, 将 $X$ 进行标准化处理, 那么 $X^TX$ 可以看作是样本的协方差矩阵. 对 $X^TX$ 进行特征值分解, 并最大的 $p$ 个特征值所对应的特征向量 $B_1,B_2,\cdots,B_p$, 则便可得到 $B^* = (B_1,B_2,\cdots,B_p)$. 因而, 对于任意的 $x^i$, 便可得到其低维表示 $z^i = B^Tx^i$.

### 1.5.2 LDA

LDA 中文叫做线性判别分析, LDA 的核心思想是设法将样本投影到一个超平面上, 且使得同类的样本投影后尽可能的接近而不同类的样本投影后尽可能的分开. LDA 与 PCA 十分相似, PCA 试图找到方差最大的主成分, 而 LDA 的目标是发现可以最优化分类的特征子空间. 与 PCA 类似, 需要将 $X$ 进行标准化处理, 然后将数据集按类别划分为 $\{C_1,C_2, \cdots, C_C\}$, 并计算各个子类所在集合的均值向量 $\mu_j, j= 1,2,\cdots, C$, 则不同类间尽可能分离等价于

$$
\displaystyle \max_A \sum_{i,j=1}^C ||A^T(\mu_i - \mu_j)||^2 =  \max_A A^T \sum_{i,j=1}^C (u_i-u_j)(u_i-u_j)^T W = \max_A A^T S_b A
$$

同类之间尽可能接近等价于

$$
\displaystyle \min_A \sum_{x \in C_i} ||A^T(x_i - \mu_i)||^2 = \min_A A^T \sum_{x \in C_i} (x-u_i)(x-u_i)^T A = \min_A A^T S_w A
$$

构造类间散度矩阵 $S_b$ 以及类内散度矩阵 $S_w$. 故而原问题便转换为以下优化问题:

$$
\displaystyle\max_A \frac{\text{Tr}(A^TS_bA)}{\text{Tr}(A^TS_wA)}
$$

其中 $A$ 相当于 PCA 中的 $B$, 被视为投影矩阵. 因而, 与 PCA 类似取 $S_w^{-1}S_b$ 前 $p$ 大的特征值所对应的特征向量组成 $A$, 最后使用 $A$ 将样本投影到新的特征子空间中.

# 实验与结果分析

## 数据集简介


# 总结与展望

# 参考文献
