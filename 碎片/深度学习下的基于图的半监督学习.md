[^1]: Wikipedia.One-hot.[DB/OL].https://en.wikipedia.org/wiki/One-hot.

[^2]: 朱文涛, 袁勇.Python计算机视觉编程[M].北京：人民邮电出版社，2014.7:152-154.

[^3]: Zhu X, Ghahramani Z.Learning from labeled and unlabeled data with label propagation.Pittsburgh: Carnegie Mellon University, 2002

[^4]: Chapelle O, Scholkopf B.Semi-Supervised Learning.MA: MIT Press, 2006

[^5]: Zhou D, Bousquet O, Lal T N, et al. Learning with Local and Global Consistency[C]. neural information processing systems, 2004: 321-328.

[^6]: 匡继昌.实分析与泛函分析[M].北京:高等教育出版社.2002.8.

# 基于 CNN 提取特征进行标签传播

摘要: a

关键词: Label Propagation; LDA; PCA; SVM; CNN

# 1 相关工作

## 1.1 研究的背景与意义

本文先是介绍了一些泛函分析和矩阵理论的几个已经定义和定理并以此为基础介绍了线性回归模型; 然后, 再以线性回归为起点拓展出线性分类和标签传播以及深度学习的概念, 接着引入了基于 CNN 提取特征用来做标签传播; 最后考虑到数据特征的冗余, 介绍了降维思想.

===近十几年，基于图的半监督学习算法如雨后春笋般喷涌而出, 包括标签传播方法, 随机游走方法, 和图正则化方法等. 它们的共通之处在于: 先构造一个图, 再将所有数据点 (包括有标记的和无标记的数据点) 看成图中的节点 (node), 数据点之间的相似程度看作是图中边 (edge) 的权重. 通过对图结构进行分析, 数据点的标记信息逐步传递到图中的其他数据点，从而为未标记的数据打上标签. 本文仅仅研究标签传播算法. 本文先是使用了一个最小化二次正则框架对几种经典的基于图的半监督学习方法进行了统一化归. 求解这个统一的优化问题相当于求解一个尺度为数据点数目的线性模型，可以得到基于图的半监督学习方法的闭合形式解. 然后, 加入了 CNN 网络结构来进一步的提升模型的准确度.===

## 1.2 符号说明

下面的 $\hat{}$ 均表示预测值. 记

$$
V_l = \text{span}(\alpha^1, \alpha^2, \cdots, \alpha^l);
V_u = \text{span}(\alpha^{l+1}, \alpha^{l+2}, \cdots, \alpha^m)
$$

其中, 对于 $1\leq i\leq l$, $\alpha^i$ 有标签 $y^i$, 对于分类任务, $y^i \in \{1, 2, \cdots, C\}$ 或者 $y^i \in {\{0,1\}}^C$ (当 $y^i$ 属于类别 $k \in \{1, 2, \cdots, C\}$ 时, $y_i$ 的第 $k$ 个元素是 $1$, 其余元素均为 $0$, 即 One-hot[^1] 编码的形式), 其中 $C$ 为类别数目.

对于 $∀ 1 \leq i\leq m$, 人们通过一些手段可以获得数据 $\alpha^i$ 的数字特征 $x$. 以一张彩色图片 $\alpha$ 为例, 人们可以获悉其像素的三维数组形式, 然后, 人们通过分析该数组, 建立一些模型来分析原数据的信息.

本文仅仅考虑图像分类任务. 具体介绍如下:

### 1.2.1 图像分类任务

对于 $∀ 1 \leq i \leq l,$ 一般有一个映射 $F,$ 使得 $y^i = F(\alpha^i)w + b = (x^i)^Tw + b$, 记
$$
X_l = \begin{pmatrix}
(x^1)^T\\(x^2)^T\\ \vdots \\(x^l)^T
\end{pmatrix};
Y_l = \begin{pmatrix}
(y^1)^T\\(y^2)^T\\ \vdots \\(y^l)^T
\end{pmatrix}
$$

则有 $\hat{Y}_l = X_lw + b$ 成立 (一般被称为**线性模型**).

为了求解的便利, 记
$$
\Phi_l = (X_l,1); \theta = \begin{pmatrix}
w\\b
\end{pmatrix}
$$
则 $\hat{Y}_l = \Phi_l \theta$.

#### 1.2.1.1 线性回归

可以说**线性回归**是大多数机器学习模型的基石. 模型的求解过程便是最小化损失函数 $J_{\theta} = \frac{1}{2}||\hat{Y}_l - Y_l||^2$ 的过程. 该损失函数可以使用**最小二乘法**求得其最小二乘解为 $\theta^* = (\Phi_l^T\Phi_l)^{\dagger}\Phi_l^TY_l$, 但若 $X_l$ 的维度很高, 计算机内存开销会很大, 此时可以选择使用**随机梯度下降法**来求解.

$$
\theta^* = 
$$

#### 1.2.1.2 分类任务

当 $y^i \in \{1, 2, \cdots, C\}$ 时, 一般采用一对其他 ($OvR$) 或 一对一($OvO$) 的分类策略, 获得多个二分类器, 测试时选择表现比较好的分类器即可.

当然, 还可以采用基于概率的方法进行分类, 此时考虑线性映射

$$
\begin{aligned}
p: \;&T \rightarrow {\{y: y\in [0,1]\}}^C\\
&x \mapsto y
\end{aligned}
$$

其中 $T = \operatorname{span}(x^1, x^2, \cdots, x^l)$, 这里 $p(x)$ 可以理解为数据对于各个类别的置信程度. 最终的预测结果是预测概率最大的分量对应的类别, 即 $\hat{y}^i = \operatorname{\arg\max} p(x)$


## 1.3 CNN

前面提到的线性模型 $f(\alpha) = x^Tw + b$, 在神经网络模型中可以这样理解:

- 将 $f_1, f_2, \cdots, f_n$ 看作神经网络的输出层的 $n$ 个神经元, 便可生成 $x = (f_1(\alpha), f_2(\alpha), \cdots, f_n(\alpha))^T = (x_1,x_2,\cdots,x_n)^T$

这样, $\hat{F}(\alpha)$ 可以是 CNN 等网络结构.

## 1.4 标签传播

下面构建一个图 ${\mathcal{G}} = (V, E)$, 其中节点集为 $V = \{x^i,y^i\}_{i=1}^l \;\cup\; \{x^j\}_{j=l+1}^{l+u}$; 边集为 $E = \{e_{ij}: e_{ij} = (x^i,x^j)\}$ ($ 1 \leq i,j \leq m$), 可表示为一个亲和矩阵（affinity matrix）(矩阵中每个元素表示两两之间的相似性分数[^3]) $W$，令 $d_j = \displaystyle\sum_{j=1}^m (W)_{ij}$, 而对角矩阵 $D = \operatorname{diag}(d_1, d_2, \cdots, d_m)$ 被称为**度矩阵**, 则由 $\Delta = D- W$ ($\Delta$ 被称为拉普拉斯矩阵) 则可以定义 $f$ 的能量函数为
$$
    E(f) = \frac{1}{2} {\displaystyle\sum_{i=1}^{m}\sum_{j=1}^m (W)_{ij} ||F_i - F_j||^2}
= \langle F\Delta,\, F \rangle = \operatorname{Tr}(FΔF^T)
$$

将矩阵分块, 得到
$$
D = \begin{bmatrix}
    D_{ll} & D_{lu}\\
    D_{ul} & D_{uu}
\end{bmatrix};
W = \begin{bmatrix}
    W_{ll} & W_{lu}\\
    W_{ul} & W_{uu}
\end{bmatrix};\Delta = \begin{bmatrix}
    \Delta_{ll} & \Delta_{lu}\\
    \Delta_{ul} & \Delta_{uu}
\end{bmatrix}
$$


故而有
$$
\begin{aligned}
    \langle F\Delta,\, F \rangle = \operatorname{Tr}(F\Delta F^T)
     &= \operatorname{Tr}(\begin{bmatrix}F_l\\ F_u\end{bmatrix} \Delta \begin{bmatrix}
    F_l^T & F_u^T
\end{bmatrix})\\
&= \operatorname{Tr}(Δ(F_l^TF_l + F_u^TF_u))\\
&= ⟨F_lΔ,F_l⟩ + ⟨F_uΔ,F_u⟩
\end{aligned}
$$
这里设 $F_l = Y_l$, 有
$$
\begin{aligned}
    ||Y - F||^2 = ||F||^2+||Y||^2 - 2⟨F,Y⟩
\end{aligned}
$$

故而原问题可以转换为:

$$
\underset{F_u, Y_u}{\arg\min}\; Loss = \underset{F_u, Y_u}{\arg\min}\; ⟨F_lΔ,F_l⟩ + ⟨F_uΔ,F_u⟩
$$

因此，由 $\frac{\partial Loss}{\partial F_u} = 0$, 可知 $F_u = F_l W_{lu} \Delta_{uu}^{-1}$.

令 $P = WD^{-1}$, 则 $F_u = F_l P_{lu} (I - P_{lu})^{-1}$.

### 关于权重矩阵 $W$ 与 图的边

权重矩阵 $W$ 又被称为**相似度矩阵**, **邻接矩阵**或**亲和矩阵**, $(W)_{ij}$ 作为样本点相似度的计算方法, 可以是样本数据 $x_i$ 与 $x_j$ 之间的距离 (如欧式距离, 马氏距离, 夹角余弦), 也可以是带宽为 $\sigma$ 的高斯核: $(W)_{ij} = exp(- \frac{||x_i - x_j||^2}{2\sigma^2}) \in [0, 1]$. 根据图中边的连接情况, 图大致可以分为全连接图 (Fully connected graph, 任意两个节点之间都有边存在), $k$ 近邻图 ($kNN$, k nearest neighbours graph) 和 $\varepsilon NN$ 图.

$k$ 近邻图需要首先知晓任意节点的 $k$ 个近邻. 记 $x_i$ $(i=1, \cdots, n)$ 最近的 $k$ 个近邻为 $N(x_i)$, 注意 $x_i \in N(x_j)$, 不一定就有 $x_j \in N(x_i)$. 因此, 若 $x_i \in N(x_j)$ 或者 $x_j \in N(x_i)$ 当中任意一个成立, 那么 $x_i$ 与 $x_j$ 之间就有边连接. 若只是节点距离 $||x_i - x_j|| < \varepsilon$ 时, 才连接这两个点, 此时的图便是 $\varepsilon NN$ 图了.



令 $\mathcal{I}$ 表示输入数据 $x$ 的数据空间, 被称为**输入空间** (input space); $\mathcal{F} =\mathcal{I}^{**}$ 被称为**特征空间** (feature space).

若 $\mathcal{I}^*$ 是数域 $K$ (实数域 $\mathbb{R}$ 或复数域 $\mathbb{C}$) 上基底为 $\phi = (\phi_1, \phi_2, \cdots, \phi_n)$ 的线性空间, $\forall \alpha_i \in \mathcal{I}$, 其中 $1 \leq n \leq \infty$, 则存在唯一的坐标 $\theta = (\theta_1,\theta_2, \cdots,\theta_n)^T \in K^n$ 使得任意的 $f \in \mathcal{I}^*$ 有
$$
f = \displaystyle\sum_{i=1}^n \phi_i \theta_i = \phi \theta
$$


## 1.5 降维

降维是指提取高维的关键信息, 将其转换为易于计算的低维空间的问题进行求解的过程.

本文主要涉及两种降维方法, 无监督的 PCA 和有监督的 LDA.

### 1.4.1 PCA

无监督降维的目的是把高维空间的数据变换到低维空间, 并使得降维后的数据尽可能保留原有的信息. 假设训练样本的特征向量集为 $\{x^{(i)}\}_{i=1}^m$, 其中 $x^{(i)} \in \mathbb{R}^n$ 一般地, 需要对特征向量进行中心化
$$
x^{(i)} ← x^{(i)} - \frac{1}{m} \displaystyle\sum_{j=1}^m x^{(j)}
$$
中心化后的 $x^{(i)}$ 仍然记作 $x^{(i)}$, 下面的讨论默认 $x^{(i)}$ 已经做了中心化处理.
记
$$
X = \begin{pmatrix}
x^{(1)}\\x^{(2)}\\ \vdots\\x^{(m)}
\end{pmatrix}
$$

PCA 中文叫做**主成分分析法**, 该方法将输入数据 $x$ 正交投影表示为 $z$, 目的是学习该正交变换, 使得降维后的数据尽可能的保留原数据空间的信息, 具体如下:

设编码函数为
$$
\begin{aligned}
f:\; & \mathbb{R}^n \rightarrow \mathbb{R}^p\\
&x \mapsto z
\end{aligned}
$$
解码函数为
$$
\begin{aligned}
g:\; & \mathbb{R}^p \rightarrow \mathbb{R}^n\\
&z \mapsto x = zD
\end{aligned}
$$
其中 $p<n$, $D \in \mathbb{R}^{p \times n}, DD^T = I_p$, 通常 $D$ 被称为解码矩阵.
为了保留原始信息, 需要做如下工作:
$$
\begin{aligned}
\hat{z} &= \underset{z}{\operatorname{\arg\min}} &||x - g(z)||^2\\
&= \underset{z}{\operatorname{\arg\min}} &||g(z)||^2 - 2 \langle x, g(z) \rangle\\
&= \underset{z}{\operatorname{\arg\min}} &||z||^2 + 2 \langle z, xD^T \rangle
\end{aligned}
$$

由 $\frac{\partial{\hat{z}}}{\partial{z}}=0$ 可得 $z = xD^T$
下面进行 PCA 重构操作:
$$
r(x) = g(f(x)) = xD^TD
$$
寻找最优的 $D$
$$
\begin{aligned}
\hat{D} &= \underset{D}{\operatorname{\arg\min}} &\displaystyle\sum_{i=1}^m ||x^{(i)} - r(x^{(i)})||^2\\
&= \underset{D}{\operatorname{\arg\min}} &||X-XD^TD||_F^2\\
&= \underset{D}{\operatorname{\arg\min}} &||XD^TD||_F^2 - 2 \langle X^TX, D^TD \rangle\\
&= \underset{D}{\operatorname{\arg\min}} & - ||XD^T||_F^2
\end{aligned}
$$
故而原问题转换为
$$
\begin{aligned}
&\hat{D} = \underset{D}{\operatorname{\arg\min}} &||XD^T||_F^2\\
&\operatorname{s.t.} & DD^T = I_p
\end{aligned}
$$

由拉格朗日乘子法将该问题转换为
$$
\hat{D} = \underset{D}{\operatorname{\arg\min}} \;- ||XD^T||_F^2 + λ(DD^T - I_p)
$$
求得问题的解为
$$
Dλ = DX^TX
$$

### 1.4.2 LDA







# 实验与结果分析



# 总结与展望

# 参考文献
